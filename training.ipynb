{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vector(feature_name, df):\n",
    "    def Jaccard(matrix):\n",
    "        matrix = np.mat(matrix)\n",
    "\n",
    "        numerator = matrix * matrix.T\n",
    "\n",
    "        denominator = (\n",
    "            np.ones(np.shape(matrix)) * matrix.T\n",
    "            + matrix * np.ones(np.shape(matrix.T))\n",
    "            - matrix * matrix.T\n",
    "        )\n",
    "\n",
    "        return numerator / denominator\n",
    "\n",
    "    all_feature = []\n",
    "    drug_list = np.array(df[feature_name]).tolist()\n",
    "    # Features for each drug, for example, when feature_name is target, drug_list=[\"P30556|P05412\",\"P28223|P46098|……\"]\n",
    "    for i in drug_list:\n",
    "        for each_feature in i.split(\"|\"):\n",
    "            if each_feature not in all_feature:\n",
    "                all_feature.append(each_feature)  # obtain all the features\n",
    "    #print(\"length of all feature is\", len(all_feature))\n",
    "    feature_matrix = np.zeros((len(drug_list), len(all_feature)), dtype=float)\n",
    "    df_feature = DataFrame(\n",
    "        feature_matrix, columns=all_feature\n",
    "    )  # Consrtuct feature matrices with key of dataframe\n",
    "    for i in range(len(drug_list)):\n",
    "        for each_feature in df[feature_name].iloc[i].split(\"|\"):\n",
    "            df_feature[each_feature].iloc[i] = 1\n",
    "\n",
    "    df_feature = np.array(df_feature)\n",
    "    sim_matrix = np.array(Jaccard(df_feature))\n",
    "    \n",
    "    #print(feature_name + \" len is:\" + str(len(sim_matrix[0])))\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "def prepare(df_drug, feature_list, mechanism, action, drugA, drugB):\n",
    "    d_label = {}\n",
    "    d_feature = {}\n",
    "\n",
    "    # Transfrom the interaction event to number\n",
    "    d_event = []\n",
    "    for i in range(len(mechanism)):\n",
    "        d_event.append(mechanism[i] + \" \" + action[i])\n",
    "\n",
    "    count = {}\n",
    "    for i in d_event:\n",
    "        if i in count:\n",
    "            count[i] += 1\n",
    "        else:\n",
    "            count[i] = 1\n",
    "    event_num = len(count)\n",
    "    list1 = sorted(count.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i in range(len(list1)):\n",
    "        d_label[list1[i][0]] = i\n",
    "\n",
    "    vector = []\n",
    "    for i in feature_list:\n",
    "        #vector = np.hstack((vector, feature_vector(i, df_drug, vector_size)))\n",
    "        vector.append(feature_vector(i, df_drug))\n",
    "    vector = np.stack(vector, axis=-1)\n",
    "    # Transfrom the drug ID to feature vector\n",
    "    for i in range(len(np.array(df_drug[\"name\"]).tolist())):\n",
    "        d_feature[np.array(df_drug[\"name\"]).tolist()[i]] = vector[i]\n",
    "\n",
    "    # Use the dictionary to obtain feature vector and label\n",
    "    new_feature = []\n",
    "    new_label = []\n",
    "\n",
    "    for i in range(len(d_event)):\n",
    "        temp = np.concatenate([d_feature[drugA[i]][None], d_feature[drugB[i]][None]], axis=0)\n",
    "        new_feature.append(temp)\n",
    "        new_label.append(d_label[d_event[i]])\n",
    "\n",
    "    new_feature = np.array(new_feature)  # 323539*....\n",
    "    new_label = np.array(new_label)  # 323539\n",
    "\n",
    "    return new_feature, new_label, event_num\n",
    "\n",
    "\n",
    "df_drug = pd.read_csv(\"drug_features.csv\")\n",
    "extraction = pd.read_csv(\"extraction.csv\")\n",
    "mechanism = extraction[\"mechanism\"]\n",
    "action = extraction[\"action\"]\n",
    "drugA = extraction[\"drugA\"]\n",
    "drugB = extraction[\"drugB\"]\n",
    "feature_list = [\"pathway\", \"target\", \"enzyme\", \"category\"]\n",
    "new_feature, new_label, event_num = prepare(df_drug, feature_list, mechanism, action, drugA, drugB)\n",
    "new_feature = torch.tensor(new_feature, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_432489/2032577051.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.features = torch.tensor(features, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DrugInteractionDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "\n",
    "# Assuming new_feature and new_label are available as numpy arrays\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "features_train, features_val, labels_train, labels_val = train_test_split(\n",
    "    new_feature, new_label, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = DrugInteractionDataset(features_train, labels_train)\n",
    "val_dataset = DrugInteractionDataset(features_val, labels_val)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNDDI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNDDI, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 64, (3, 1), padding=(1, 0))\n",
    "        self.conv2 = nn.Conv2d(64, 128, (3, 1), padding=(1, 0))\n",
    "        self.conv3_1 = nn.Conv2d(128, 128, (3, 1), padding=(1, 0))\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, (3, 1), padding=(1, 0))\n",
    "        self.conv4 = nn.Conv2d(128, 256, (3, 1), padding=(1, 0))\n",
    "        self.fc1 = nn.Linear(256 * 572 * 4, 256)  # Adjust feature_size based on your input dimensions\n",
    "        self.fc2 = nn.Linear(256, 65)  # Assuming 65 DDI types\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.2)\n",
    "        identity = x\n",
    "        x = F.leaky_relu(self.conv3_1(x), negative_slope=0.2)\n",
    "        x = self.conv3_2(x)\n",
    "        x += identity\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "        x = F.leaky_relu(self.conv4(x), negative_slope=0.2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.2)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    val_accuracy = correct / len(val_loader.dataset)\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, filename):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, filename)\n",
    "    print(f\"Saved checkpoint: {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class focal_loss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super(focal_loss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        labels = labels[..., None]\n",
    "\n",
    "        preds_logsoft = F.log_softmax(preds, dim=1)\n",
    "        preds_softmax = torch.exp(preds_logsoft)\n",
    "\n",
    "        preds_softmax = preds_softmax.gather(1, labels)\n",
    "        preds_logsoft = preds_logsoft.gather(1, labels)\n",
    "\n",
    "        loss = -torch.mul(torch.pow((1 - preds_softmax), self.gamma), preds_logsoft)\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:21<17:16, 21.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.8764, Validation Loss: 0.3628, Validation Accuracy: 0.7918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:42<16:55, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.2777, Validation Loss: 0.3381, Validation Accuracy: 0.8103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [01:03<16:40, 21.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.2173, Validation Loss: 0.2545, Validation Accuracy: 0.8415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:25<16:21, 21.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.1825, Validation Loss: 0.2843, Validation Accuracy: 0.8480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:46<15:56, 21.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.1802, Validation Loss: 0.3486, Validation Accuracy: 0.8111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [02:07<15:33, 21.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.1394, Validation Loss: 0.2700, Validation Accuracy: 0.8332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [02:28<15:09, 21.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.1302, Validation Loss: 0.2829, Validation Accuracy: 0.8444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [02:49<14:46, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.1259, Validation Loss: 0.7220, Validation Accuracy: 0.7724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [03:10<14:25, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.1296, Validation Loss: 0.2329, Validation Accuracy: 0.8791\n",
      "Epoch 10, Train Loss: 0.1721, Validation Loss: 0.3235, Validation Accuracy: 0.8584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [03:32<14:15, 21.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: checkpoints/cnn_ddi_epoch_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [03:53<13:49, 21.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 0.0829, Validation Loss: 0.2793, Validation Accuracy: 0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [04:14<13:24, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 0.0824, Validation Loss: 0.3565, Validation Accuracy: 0.8449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [04:35<12:59, 21.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 0.1575, Validation Loss: 0.2600, Validation Accuracy: 0.8799\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNDDI().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = focal_loss()\n",
    "\n",
    "num_epochs = 50\n",
    "checkpoint_interval = 10\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy = validate(model, device, val_loader, criterion)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    if (epoch + 1) % checkpoint_interval == 0 or (epoch + 1) == num_epochs:\n",
    "        save_checkpoint(model, optimizer, epoch + 1, f'checkpoints/cnn_ddi_epoch_{epoch+1}.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
